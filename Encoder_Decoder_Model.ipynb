{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6zYEJDIadCe4PG7WuOdUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshu-Naik/Gen_AI/blob/main/Encoder_Decoder_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rhPSEj0bmAqL"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd, string\n",
        "from string import digits\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIpQR79LJeYB",
        "outputId": "5ab3785e-8a77-4123-b947-941a3c4b3eb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset in streaming mode\n",
        "dataset = load_dataset(\n",
        "    \"cfilt/iitb-english-hindi\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "samples = []\n",
        "max_samples = 25000\n",
        "\n",
        "for i, example in enumerate(dataset):\n",
        "    if i >= max_samples:\n",
        "        break\n",
        "\n",
        "    samples.append({\n",
        "        \"english_sentence\": example[\"translation\"][\"en\"],\n",
        "        \"hindi_sentence\": example[\"translation\"][\"hi\"]\n",
        "    })\n",
        "\n",
        "# Convert only the collected samples to DataFrame\n",
        "lines = pd.DataFrame(samples)\n",
        "\n",
        "# Optional: clean\n",
        "lines = lines.dropna().drop_duplicates()\n",
        "\n",
        "print(lines.head())\n",
        "print(lines.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6-A5xmstJiAk",
        "outputId": "e5f71a9d-c909-4ac4-e414-85e3fa81ed33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 english_sentence  \\\n",
            "0  Give your application an accessibility workout   \n",
            "1               Accerciser Accessibility Explorer   \n",
            "2  The default plugin layout for the bottom panel   \n",
            "3     The default plugin layout for the top panel   \n",
            "4  A list of plugins that are disabled by default   \n",
            "\n",
            "                                      hindi_sentence  \n",
            "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें  \n",
            "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक  \n",
            "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका  \n",
            "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका  \n",
            "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...  \n",
            "(5174, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cleaning**\n",
        "\n",
        "Here it remove punctuation and digits and converts text to lowercase and strips whitespace.\n",
        "\n",
        "It applies Applies cleaning and adds special tokens to Hindi sentences to mark start and end (start_, _end)."
      ],
      "metadata": {
        "id": "QTCgdyfhi_Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  exclude = set(string.punctuation)\n",
        "  text = ''.join(ch for ch in text if ch not in exclude)\n",
        "  text = text.translate(str.maketrans('', '', digits))\n",
        "  text = text.strip().lower()\n",
        "  return text\n",
        "\n",
        "lines['english_sentence'] = lines['english_sentence'].apply(clean_text)\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(clean_text)\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: 'START_ ' + x + ' _END')"
      ],
      "metadata": {
        "id": "uw5Lr3qXMU8N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n",
        "Converts text to sequences of integers using word indices. Hindi tokenizer keeps_because of special tokens."
      ],
      "metadata": {
        "id": "S3f47XjujRXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = Tokenizer()\n",
        "eng_tokenizer.fit_on_texts(lines['english_sentence'])\n",
        "eng_seq = eng_tokenizer.texts_to_sequences(lines['english_sentence'])\n",
        "\n",
        "hin_tokenizer = Tokenizer(filters='')\n",
        "hin_tokenizer.fit_on_texts(lines['hindi_sentence'])\n",
        "hin_seq = hin_tokenizer.texts_to_sequences(lines['hindi_sentence'])"
      ],
      "metadata": {
        "id": "-KDlI9g_NQLo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding**\n",
        "\n",
        "Pads sequences to uniform length\n",
        "\n",
        "decoder_target is shifted version of decoder_input used for teacher forcing."
      ],
      "metadata": {
        "id": "BJmzEKZ6jb0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_len = max([len(x) for x in eng_seq])\n",
        "max_hin_len = max([len(x) for x in hin_seq])\n",
        "\n",
        "encoder_input = pad_sequences(eng_seq, maxlen=max_eng_len, padding='post')\n",
        "decoder_input = pad_sequences(hin_seq, maxlen=max_hin_len, padding='post')\n",
        "\n",
        "decoder_target = np.zeros((decoder_input.shape[0], decoder_input.shape[1], 1))\n",
        "decoder_target[:, 0:-1, 0] = decoder_input[:, 1:]"
      ],
      "metadata": {
        "id": "bA2NonjVNxDo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Model Architecture\n",
        "Encoder:**\n",
        "\n",
        "It embeds English input and Passes through LSTM. Keeps hidden (state_h) and cell state (state_c) to pass to decoder."
      ],
      "metadata": {
        "id": "XonXYzPKjtwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(eng_vocab_size, latent_dim)(encoder_inputs)\n",
        "enc_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(enc_emb)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "Xp2B-pnfOkDs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder:**\n",
        "\n",
        "It embeds Hindi input. Uses initial states from encoder and Outputs probability distribution over Hindi vocabulary at each time step."
      ],
      "metadata": {
        "id": "ERqHBPHxj0Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(hin_vocab_size, latent_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(hin_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "IW4Nj-2sRYRl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile and Train**\n",
        "\n",
        "Trains on source (encoder_input) and target (decoder_input) with shifted targets and uses RMSProp optimizer and cross-entropy loss."
      ],
      "metadata": {
        "id": "0oymTfgTj8-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit([encoder_input, decoder_input], decoder_target, batch_size=64, epochs=20, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbnQ6guAR5rm",
        "outputId": "be415f7d-6587-4f11-f930-858f9dde7474"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.8134 - loss: 2.4078 - val_accuracy: 0.9172 - val_loss: 0.6539\n",
            "Epoch 2/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.8938 - loss: 0.7100 - val_accuracy: 0.9172 - val_loss: 0.6092\n",
            "Epoch 3/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2s/step - accuracy: 0.9009 - loss: 0.6549 - val_accuracy: 0.9172 - val_loss: 0.6089\n",
            "Epoch 4/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.9006 - loss: 0.6462 - val_accuracy: 0.9175 - val_loss: 0.6011\n",
            "Epoch 5/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.9040 - loss: 0.6205 - val_accuracy: 0.9175 - val_loss: 0.6066\n",
            "Epoch 6/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 1s/step - accuracy: 0.8994 - loss: 0.6438 - val_accuracy: 0.9175 - val_loss: 0.6075\n",
            "Epoch 7/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.9015 - loss: 0.6297 - val_accuracy: 0.9175 - val_loss: 0.6109\n",
            "Epoch 8/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.9053 - loss: 0.6047 - val_accuracy: 0.9175 - val_loss: 0.6076\n",
            "Epoch 9/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.9036 - loss: 0.6087 - val_accuracy: 0.9175 - val_loss: 0.6120\n",
            "Epoch 10/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.9078 - loss: 0.5823 - val_accuracy: 0.9175 - val_loss: 0.6049\n",
            "Epoch 11/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - accuracy: 0.9077 - loss: 0.5811 - val_accuracy: 0.9175 - val_loss: 0.6053\n",
            "Epoch 12/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.9066 - loss: 0.5829 - val_accuracy: 0.9170 - val_loss: 0.6061\n",
            "Epoch 13/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.9056 - loss: 0.5892 - val_accuracy: 0.9173 - val_loss: 0.6041\n",
            "Epoch 14/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.9099 - loss: 0.5598 - val_accuracy: 0.9173 - val_loss: 0.6007\n",
            "Epoch 15/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.9127 - loss: 0.5464 - val_accuracy: 0.9172 - val_loss: 0.6032\n",
            "Epoch 16/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.9088 - loss: 0.5617 - val_accuracy: 0.9173 - val_loss: 0.6049\n",
            "Epoch 17/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.9093 - loss: 0.5571 - val_accuracy: 0.9171 - val_loss: 0.6005\n",
            "Epoch 18/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.9089 - loss: 0.5510 - val_accuracy: 0.9174 - val_loss: 0.6024\n",
            "Epoch 19/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.9124 - loss: 0.5324 - val_accuracy: 0.9171 - val_loss: 0.6001\n",
            "Epoch 20/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - accuracy: 0.9138 - loss: 0.5266 - val_accuracy: 0.9164 - val_loss: 0.6114\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ea10625f8f0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference Models**\n",
        "\n",
        "To translate new sentences after training:"
      ],
      "metadata": {
        "id": "6CdgtC6PkFcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model_inf = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "dec_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_outputs2 = decoder_dense(dec_outputs2)\n",
        "\n",
        "decoder_model_inf = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2, state_h2, state_c2])"
      ],
      "metadata": {
        "id": "0yiVhppgd9Bh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reverse Lookup**\n",
        "\n",
        "Used to convert indices back to words during decoding."
      ],
      "metadata": {
        "id": "22N5zTmzkMwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rev_eng = {v: k for k, v in eng_tokenizer.word_index.items()}\n",
        "rev_hin = {v: k for k, v in hin_tokenizer.word_index.items()}"
      ],
      "metadata": {
        "id": "-Y_z_qXAe9oc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translate Function**\n",
        "\n",
        "It prepares input sentence. Starts decoding with <start> token and Iteratively predicts next word and feeds it back until <end> is predicted. and the test the model with example"
      ],
      "metadata": {
        "id": "CH_vvS8ikSc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentences):\n",
        "  sentences = clean_text(sentences)\n",
        "  eng_seq = eng_tokenizer.texts_to_sequences([sentences])\n",
        "  eng_seq = pad_sequences(eng_seq, maxlen=max_eng_len, padding='post')\n",
        "  state_values = encoder_model_inf.predict(eng_seq, verbose=0)\n",
        "\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  target_seq[0, 0] = hin_tokenizer.word_index['start_']\n",
        "\n",
        "  decoded = []\n",
        "  while True:\n",
        "    output_seq, h, c = decoder_model_inf.predict([target_seq] + state_values, verbose=0)\n",
        "    pred_word_ind = np.argmax(output_seq[0, -1, :])\n",
        "\n",
        "    # Handle the case where the model predicts the padding index (0)\n",
        "    if pred_word_ind == 0:\n",
        "      break\n",
        "\n",
        "    pred_word = rev_hin[pred_word_ind]\n",
        "\n",
        "    if pred_word == '_END' or len(decoded) >= max_hin_len:\n",
        "      break\n",
        "\n",
        "    decoded.append(pred_word)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = pred_word_ind\n",
        "    state_values = [h , c]\n",
        "\n",
        "  return ' '.join(decoded)\n",
        "\n",
        "print(\"English: And\")\n",
        "print(\"Hindi: \", translate(\"And\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47R8bhf0fFNl",
        "outputId": "0daca41f-ec18-4c9d-c2ed-d6c013091de2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: And\n",
            "Hindi:  a _end\n"
          ]
        }
      ]
    }
  ]
}