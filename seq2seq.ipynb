{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTQ+XIGhjrKfe9L1pOiU30",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshu-Naik/Gen_AI/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-by-Step Seq2Seq Implementation\n"
      ],
      "metadata": {
        "id": "Q6JBlUksL3vy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "94OFBHWeLopT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Encoder\n",
        "\n",
        "We will define:\n",
        "\n",
        "Each input token is converted to a dense vector (embedding).\n",
        "\n",
        "The GRU processes the sequence one token at a time, updating its hidden state.\n",
        "\n",
        "The final hidden state is returned as the context vector, summarizing the input sequence."
      ],
      "metadata": {
        "id": "PV_QdDH0QMFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim, hidden_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    output, hidden = self.rnn(embedded)\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "LRdowlrwMZAI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Decoder\n",
        "\n",
        "We will define the decoder:\n",
        "\n",
        "Takes the current input token and converts it to an embedding.\n",
        "\n",
        "GRU uses the previous hidden state (or context vector initially) to compute the new hidden state.\n",
        "\n",
        "The output is passed through a linear layer to get predicted token probabilities."
      ],
      "metadata": {
        "id": "Do2OrZVgQQ69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim + hidden_dim, hidden_dim)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    x = x.unsqueeze(0)\n",
        "    embedded = self.embedding(x)\n",
        "    output, hidden = self.rnn(embedded, hidden)\n",
        "    prediction = self.fc(output.squeeze(0))\n",
        "    return prediction, hidden"
      ],
      "metadata": {
        "id": "iSkTsTFnM0Mm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Seq2Seq Model with Teacher Forcing\n",
        "\n",
        "Batch size & vocab size: extracted from input and decoder.\n",
        "\n",
        "Encoding: input sequence → encoder → context vector (hidden).\n",
        "\n",
        "Start token: initialize decoder with token 0.\n",
        "\n",
        "Loop over max_len:\n",
        "\n",
        "Decoder predicts next token.\n",
        "\n",
        "top1 → token with max probability.\n",
        "\n",
        "Append top1 to outputs.\n",
        "\n",
        "Teacher forcing: sometimes feed true target token instead of prediction.\n",
        "\n",
        "Return predictions: concatenated sequence of token IDs."
      ],
      "metadata": {
        "id": "d6AoUkWmQZK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class seq2seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, src, trg = None, max_len = 10, teacher_forcing_ratio = 0.5):\n",
        "    batch_size = src.shape[1]\n",
        "    trg_len = self.decoder.fc.out_features\n",
        "    outputs = []\n",
        "\n",
        "    hidden = self.encoder(src)\n",
        "\n",
        "    input = torch.zeros(batch_size, dtype = torch.long).to(self.device)\n",
        "\n",
        "    for t in range(max_len):\n",
        "      output, hidden = self.decoder(input, hidden)\n",
        "      top1 = output.argmax(1)\n",
        "      outputs.append(top1.unsqueeze(0))\n",
        "\n",
        "      if trg is not None and t < trg_len and torch.rand(1) < teacher_forcing_ratio:\n",
        "        input = trg[t]\n",
        "      else:\n",
        "        input = top1\n",
        "\n",
        "    outputs = torch.cat(outputs, dim = 0)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "OQYiQ7I-Ni8J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Usage Example with Outputs\n",
        "\n",
        "Test with example,\n",
        "\n",
        "src: random input token IDs.\n",
        "\n",
        "trg: random target token IDs (used for teacher forcing).\n",
        "\n",
        "outputs: predicted token IDs for each sequence.\n",
        "\n",
        ".T: transpose to show batch sequences as rows."
      ],
      "metadata": {
        "id": "pwaTXkdEQmmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "VOCAB_SIZE = 10\n",
        "EMB_DIM = 8\n",
        "HID_DIM = 16\n",
        "SEQ_LEN = 5\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "enc = Encoder(VOCAB_SIZE, EMB_DIM, HID_DIM)\n",
        "dec = Decoder(VOCAB_SIZE, EMB_DIM, HID_DIM)\n",
        "model = seq2seq(enc, dec, device).to(device)\n",
        "\n",
        "src = torch.randint(1, VOCAB_SIZE, (SEQ_LEN, BATCH_SIZE)).to(device)\n",
        "trg = torch.randint(1, VOCAB_SIZE, (SEQ_LEN, BATCH_SIZE)).to(device)\n",
        "\n",
        "outputs = model(src, trg, max_len=SEQ_LEN, teacher_forcing_ratio=0.7)\n",
        "\n",
        "print(\"Source sequence (input tokens):\")\n",
        "print(src.T)\n",
        "print(\"\\nTarget sequence (true tokens):\")\n",
        "print(trg.T)\n",
        "print(\"\\nPredicted sequence (model output tokens):\")\n",
        "print(outputs.T)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaJqoKyQO0q6",
        "outputId": "cf2730ef-93cc-487f-aa99-667f415d3710"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source sequence (input tokens):\n",
            "tensor([[5, 9, 8, 7, 7],\n",
            "        [9, 8, 6, 4, 8]])\n",
            "\n",
            "Target sequence (true tokens):\n",
            "tensor([[4, 1, 2, 2, 8],\n",
            "        [9, 8, 1, 8, 9]])\n",
            "\n",
            "Predicted sequence (model output tokens):\n",
            "tensor([[5, 4, 7, 5, 4],\n",
            "        [4, 0, 4, 4, 4]])\n"
          ]
        }
      ]
    }
  ]
}