{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjIQSskejh3s39cQPZK/+G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshu-Naik/Gen_AI/blob/main/Self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PhWh3AdL37H-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Weight Matrix**\n",
        "\n",
        "These lines initialize random weight matrices for queries (W_q), keys (W_k) and values (W_v). In real models these are learnable parameters used to project the input into Q, K, and V representations.\n",
        "\n"
      ],
      "metadata": {
        "id": "07ehRIgACZa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Q, K, V matrices**\n",
        "\n",
        "These lines project the input X into query (Q), key (K) and value (V) matrices by multiplying with their respective weights. This transforms the input into different views used for computing attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "iU5Ou3KCCezz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Attention scores and weights**\n",
        "\n",
        "This computes the final output by weighting the values (V) with the attention scores, aggregating relevant information from the sequence. It then returns both the attention output and the attention weights for further use or analysis"
      ],
      "metadata": {
        "id": "R3atwiNhCiBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention(X):\n",
        "  batch_size, seq_len, d_model = X.shape\n",
        "  d_k = d_model\n",
        "\n",
        "  d_model = 512 # Example: dimension of the model. Adjust as needed.\n",
        "  d_k = d_model\n",
        "  W_q = np.random.randn(d_model, d_k)\n",
        "  W_k = np.random.randn(d_model, d_k)\n",
        "  W_v = np.random.randn(d_model, d_k)\n",
        "\n",
        "  batch_size = 1\n",
        "  seq_len = 10\n",
        "  X = np.random.randn(batch_size, seq_len, d_model) # Initialize X with random values\n",
        "  Q = X @ W_q\n",
        "  K = X @ W_k\n",
        "  V = X @ W_v\n",
        "\n",
        "  attention_scores = (Q @ K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
        "  attention_weight = softmax(attention_scores, axis=-1)\n",
        "\n",
        "  output = attention_weight @ V\n",
        "\n",
        "  return output, attention_weight"
      ],
      "metadata": {
        "id": "tkseeKP95K02"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.randn(1, 10, 512)\n",
        "output, weight = self_attention(X)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Weight shape:\", weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFgjQfBeADjz",
        "outputId": "af384373-df18-47b8-973c-1c3ea1c24994"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (1, 10, 512)\n",
            "Weight shape: [[[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "   0.00000000e+000 0.00000000e+000 1.00000000e+000 3.46399638e-025\n",
            "   1.57950706e-254 0.00000000e+000]\n",
            "  [7.58593788e-104 3.88952974e-009 3.26240731e-101 0.00000000e+000\n",
            "   1.58523910e-207 5.90818524e-297 9.99999996e-001 2.23849305e-117\n",
            "   0.00000000e+000 1.96130604e-196]\n",
            "  [6.67715368e-125 2.80699083e-009 1.63800641e-299 9.99999997e-001\n",
            "   0.00000000e+000 3.61231515e-310 1.80007562e-047 0.00000000e+000\n",
            "   4.02731715e-273 0.00000000e+000]\n",
            "  [1.00000000e+000 0.00000000e+000 8.85626228e-229 5.05317228e-161\n",
            "   1.22823634e-287 2.27833297e-313 1.16877719e-109 8.77170087e-192\n",
            "   1.53742319e-122 2.01390706e-178]\n",
            "  [0.00000000e+000 2.35830049e-270 0.00000000e+000 1.00000000e+000\n",
            "   1.02363538e-242 0.00000000e+000 0.00000000e+000 3.48003662e-030\n",
            "   0.00000000e+000 0.00000000e+000]\n",
            "  [0.00000000e+000 0.00000000e+000 1.90398530e-204 3.28684407e-267\n",
            "   0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "   1.00000000e+000 2.49956810e-268]\n",
            "  [0.00000000e+000 0.00000000e+000 1.12805336e-290 7.54582217e-209\n",
            "   1.00000000e+000 0.00000000e+000 2.90599794e-295 1.80375529e-194\n",
            "   2.44633052e-214 0.00000000e+000]\n",
            "  [0.00000000e+000 6.43645198e-149 0.00000000e+000 0.00000000e+000\n",
            "   0.00000000e+000 1.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "   0.00000000e+000 2.03027137e-177]\n",
            "  [0.00000000e+000 0.00000000e+000 3.44504726e-295 1.84606023e-228\n",
            "   1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "   0.00000000e+000 5.48069342e-263]\n",
            "  [0.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
            "   0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "   0.00000000e+000 0.00000000e+000]]]\n"
          ]
        }
      ]
    }
  ]
}