{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8DL8ICE15NZmUsWhyjpCB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshu-Naik/Gen_AI/blob/main/Multihead_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaled Dot-Product Attention Function**\n",
        "\n",
        "Q, K, V are queries, keys, values derived from the same source in self-attention.\n",
        "\n",
        "It results in values i.e the weighted sum for each position and head.\n",
        "\n",
        "Softmax ensures the attention weights sum to 1.\n",
        "\n",
        "If masking, irrelevant positions (like future tokens or padding) get large negative values in logits, so after softmax attention there is 0"
      ],
      "metadata": {
        "id": "FSuMDKHuF-D7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Head Attention Class**\n",
        "\n",
        "Every step mimics the original Transformer:\n",
        "\n",
        "Project to QKV,\n",
        "\n",
        "Reshape for multiple heads,\n",
        "\n",
        "Split into Q, K, V,\n",
        "\n",
        "Compute attention,\n",
        "\n",
        "Concatenate heads,\n",
        "\n",
        "Linear output.\n"
      ],
      "metadata": {
        "id": "iaJ4P7ZcGWLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.input_dim = input_dim\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # For efficiency, compute Q, K, V for all heads at once with a single linear layer\n",
        "        self.qkv_layer = nn.Linear(input_dim, 3 * d_model)\n",
        "         # Final projection, combines all heads' outputs\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, input_dim = x.size()\n",
        "\n",
        "        # Step 1: Project x into concatenated q, k, v for ALL heads at once\n",
        "        qkv = self.qkv_layer(x)\n",
        "\n",
        "        # Step 2: reshape into (batch, seq_len, num_heads, 3 * head_dim)\n",
        "        qkv = qkv.reshape(batch_size, seq_len, self.num_heads, 3 * self.head_dim)\n",
        "\n",
        "        # Step 4: Split the last dimension into q, k, v (each get last dimension of head_dim)\n",
        "        q, k, v = qkv.chunk(3, dim=-1) # Each: (batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "        # Permute q, k, v to (batch, num_heads, seq_len, head_dim)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        # Step 5: Apply scaled dot product attention\n",
        "        values, attention = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        # Step 6: Merge the heads (permute before reshape)\n",
        "        values = values.permute(0, 2, 1, 3)\n",
        "        values = values.reshape(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "\n",
        "        # Step 7: Final linear layer\n",
        "        output = self.linear_layer(values)\n",
        "        return output\n",
        "\n",
        "input_dim = 1024\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "batch_size = 30\n",
        "seq_len = 5\n",
        "\n",
        "x = torch.randn((batch_size, seq_len, input_dim))\n",
        "multihead_attn = MultiHeadAttention(input_dim, d_model, num_heads)\n",
        "output = multihead_attn.forward(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoyzUjukDnhY",
        "outputId": "fadcff7f-acde-4dcf-ff59-6a9c7091c39e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 5, 512])\n"
          ]
        }
      ]
    }
  ]
}